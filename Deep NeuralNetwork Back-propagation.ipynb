{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Utils:\n    @staticmethod\n    def sigmoid(Z): \n        A = 1/(1+np.exp(-Z))\n        cache = Z\n        return A, cache\n    \n    @staticmethod\n    def relu(Z):\n        A = np.maximum(0,Z)\n        cache = Z \n        return A, cache\n    \n    @staticmethod\n    def relu_backward(dA, cache):\n        Z = cache\n        dZ = np.array(dA, copy=True)\n        dZ[Z <= 0] = 0\n        return dZ\n    \n    @staticmethod\n    def sigmoid_backward(dA, cache):\n        Z = cache\n        s = 1/(1+np.exp(-Z))\n        dZ = dA * s * (1-s) \n        return dZ","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DNN(Utils):\n    \n    '''\n    input:\n        X: input features\n        y: target\n        L: (array-like) number of hidden layers with units \n        lr: learning rate\n        \n        example:\n        import numpy as np\n        features = np.random.randn(100,8)\n        targets = np.random.randn(100,1)\n        L = [16,32,32,64]\n        lr = 1e-3\n        model = DNN(X = features,y=targets,layers=L,learning_rate=lr)\n    '''\n    \n    def __init__(self,X,y,layers,learning_rate):\n        super().__init__()\n        self.x = X\n        self.y = y\n        self.L = layers\n        self.lr = learning_rate\n    \n    def initialize_parameters(self):\n        layer = len(self.L)\n        n = self.x.shape[1]\n        parameters = {}\n        for l in range(1,layer):\n            parameters[\"W\"+str(l)] = np.random.randn(self.L[l],self.L[l-1]) * 1/np.sqrt(n ** (layer-1))\n            parameters[\"b\"+str(l)] = np.zeros((self.L[l],1))\n        \n        return parameters\n    \n    def linear_forward(self,A,W,b):\n        Z = np.dot(W,A) + b\n        cache = (A,W,b)\n        return Z,cache\n    \n    def linear_activation_forward(self,A_prev,W,b,activation):\n        if activation == \"sigmoid\":\n            Z, linear_cache = self.linear_forward(A_prev, W, b)\n            A,activation_cache = Utils.sigmoid(Z)\n        elif activation == \"relu\":\n            Z, linear_cache = self.linear_forward(A_prev, W, b)\n            A,activation_cache = Utils.relu(Z)\n        \n        cache = (linear_cache,activation_cache)\n        return A,cache\n    \n    def forward(self,parameters):\n        caches = []\n        A = self.x\n        L = len(parameters)//2\n        \n        for l in range(1,L):\n            A_prev = A\n            A,cache = self.linear_activation_forward(A_prev,\n                                               parameters[\"W\"+str(l)],\n                                               parameters[\"b\"+str(l)],\n                                               activation = \"relu\")\n            caches.append(cache)\n        \n        AL,cache = self.linear_activation_forward(A,\n                                             parameters[\"W\"+str(L)],\n                                            parameters[\"b\"+str(L)],\n                                            activation = \"sigmoid\")\n        caches.append(cache)\n        \n        return AL,caches\n    \n    def compute_cost(self,AL):\n        Y = self.y\n        m = Y.shape[1]\n        cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n        cost = np.squeeze(cost)\n        return cost\n    \n    def linear_backward(self,dZ,cache):\n        A_prev, W, b = cache\n        m = A_prev.shape[1]\n        dW = 1/m*np.dot(dZ ,A_prev.T)\n        db = 1/m*np.sum(dZ , axis = 1, keepdims=True)\n        dA_prev = np.dot(W.T ,dZ)\n        return dA_prev, dW, db\n    \n    def linear_activation_backward(dA, cache, activation):\n        linear_cache, activation_cache = cache\n        if activation == \"relu\":\n            dZ = Utils.relu_backward(dA, activation_cache)\n            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n        elif activation == \"sigmoid\":\n            dZ = Utils.sigmoid_backward(dA, activation_cache)\n            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n        return dA_prev, dW, db\n    \n    def backward(self,AL,caches):\n        Y = self.y\n        grads = {}\n        m = AL.shape[1]\n        L = len(caches)\n        Y = Y.reshape(AL.shape)\n        \n        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n        current_cache = caches[-1]\n        grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.linear_backward(Utils.sigmoid_backward(dAL, \n                                                                                                        current_cache[1]), \n                                                                                                        current_cache[0])\n        for l in reversed(range(L-1)):\n            current_cache = caches[l]\n            dA_prev_temp, dW_temp, db_temp = self.linear_backward(Utils.sigmoid_backward(dAL,\n                                                                              current_cache[1]),\n                                                             current_cache[0])\n            grads[\"dA\" + str(l + 1)] = dA_prev_temp\n            grads[\"dW\" + str(l + 1)] = dW_temp\n            grads[\"db\" + str(l + 1)] = db_temp\n            \n        return grads\n    \n    def update_parameters(self,parameters,grads):\n        learning_rate = self.lr\n        L = len(parameters) // 2\n        for l in range(L):\n            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        return parameters\n    \n    def training(self,iterations=100,print_cost=False,print_interval=10):\n        parameters = self.initialize_parameters()\n        costs = []\n        for i in tqdm_notebook(range(0,iterations)):\n            AL,caches = self.forward(parameters)\n            cost = self.compute_cost(AL)\n            grads = self.backward(AL,caches)\n            parameters = self.update_parameters(parameters,grads)\n            if print_cost & i % print_interval:\n                print(f'cost after interval {i}: {cost}')\n                costs.append(cost)\n        return costs","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import fetch_california_housing\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ncal_housing = fetch_california_housing()\nX = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\ny = cal_housing.target","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.reshape(-1,1)\n\ny -= y.mean()","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layers_dims = [20640, 20, 7, 5, 1]","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DNN(X,y,layers=layers_dims,learning_rate=1e-2)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = model.initialize_parameters()","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(parameters[\"W1\"].shape)\nprint(parameters[\"W2\"].shape)\nprint(parameters[\"W3\"].shape)\nprint(parameters[\"W4\"].shape)","execution_count":58,"outputs":[{"output_type":"stream","text":"(20, 20640)\n(7, 20)\n(5, 7)\n(1, 5)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AL,cache = model.forward(parameters)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grads = model.backward(AL,cache)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}